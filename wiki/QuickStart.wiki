=QuickStart=

===Installing Abot===
  #: Download the latest from the Downloads tab
  #: Extract the Abot.dll file and add a reference from your project
  #: Nuget package coming soon

===Using Abot===
  #: Add the following using statements.. 
  {{{
  using Abot.Crawler;
  using Abot.Core;
  using Abot.Poco;
  }}}
  #: Do one of the following
    #: Add the following to the app.config or web.cofig of the assembly using the library
    {{{
<?xml version="1.0"?>
<configuration>
  <configSections>
    <section name="abot" type="Abot.Core.ConfigurationSectionHandler, Abot"/>
  </configSections>

  <abot>
    <crawlBehavior
      maxConcurrentThreads="10"
      maxPagesToCrawl="1000"
      userAgentString="abot v1.0 http://code.google.com/p/abot"
      crawlTimeoutSeconds="0"
      downloadableContentTypes="text/html"
      isUriRecrawlingEnabled="false"
      isExternalPageCrawlingEnabled="true"
      isExternalPageLinksCrawlingEnabled="false"
        />
    <politeness
      isThrottlingEnabled="false"
      minCrawlDelayPerDomainMilliSeconds="0"
      />
    <extensionValues>
      <add key="key1" value="value1" />
      <add key="key2" value="value2" />
    </extensionValues>
  </abot>  
</configuration>
    }}}
    #: Create an instance of Abot.Poco.CrawlConfiguration (this overrides web/app.config)
    {{{
CrawlConfiguration config = new CrawlConfiguration
{
	CrawlTimeoutSeconds = 3600,
	DownloadableContentTypes = "text/html, text/plain",
	IsExternalPageCrawlingEnabled = false,
	IsExternalPageLinksCrawlingEnabled  = false,
	MaxConcurrentThreads = 10,
	MaxPagesToCrawl = 1000,
};
    }}}
  #: Create an instance of Abot.Crawler.!WebCrawler
  {{{
WebCrawler crawler = new WebCrawler(); //If you configured using web/app.config
  }}}
  {{{
WebCrawler crawler = new WebCrawler(config); //if you manually created CrawlConfiguration object, this completely overrides web/app.config
  }}}
  #: Register for events and create processing methods
{{{
crawler.PageCrawlStartingAsync += crawler_ProcessPageCrawlStarting;
crawler.PageCrawlCompletedAsync += crawler_ProcessPageCrawlCompleted;
crawler.PageCrawlDisallowedAsync += crawler_PageCrawlDisallowed;
crawler.PageLinksCrawlDisallowedAsync += crawler_PageLinksCrawlDisallowed;
}}}
{{{
void crawler_ProcessPageCrawlStarting(object sender, PageCrawlStartingArgs e)
{
	PageToCrawl pageToCrawl = e.PageToCrawl;
	Console.WriteLine("About to crawl link {0} which was found on page {1}", pageToCrawl.Uri.AbsoluteUri, pageToCrawl.ParentUri.AbsoluteUri);
}

void crawler_ProcessPageCrawlCompleted(object sender, PageCrawlCompletedArgs e)
{
	CrawledPage crawledPage = e.CrawledPage;

	if (crawledPage.WebException != null || crawledPage.HttpWebResponse.StatusCode != HttpStatusCode.OK)
		Console.WriteLine("Crawl of page failed {0}", crawledPage.Uri.AbsoluteUri);
	else
		Console.WriteLine("Crawl of page succeeded {0}", crawledPage.Uri.AbsoluteUri);

	if (string.IsNullOrEmpty(crawledPage.RawContent))
		Console.WriteLine("Page had no content {0}", crawledPage.Uri.AbsoluteUri);
}

void crawler_PageLinksCrawlDisallowed(object sender, PageLinksCrawlDisallowedArgs e)
{
	CrawledPage crawledPage = e.CrawledPage;
	Console.WriteLine("Did not crawl the links on page {0} due to {1}", crawledPage.Uri.AbsoluteUri, e.DisallowedReason);
}

void crawler_PageCrawlDisallowed(object sender, PageCrawlDisallowedArgs e)
{
	PageToCrawl pageToCrawl = e.PageToCrawl;
	Console.WriteLine("Did not crawl page {0} due to {1}", pageToCrawl.Uri.AbsoluteUri, e.DisallowedReason);
}
}}}
  #: Run the crawl and check the crawl result
{{{
CrawlResult result = crawler.Crawl(new Uri("http://localhost:1111/"));

if (result.ErrorOccurred)
	Console.WriteLine("Crawl of {0} completed with error: {1}", result.RootUri.AbsoluteUri, result.ErrorMessage);
else
	Console.WriteLine("Crawl of {0} completed without error.", result.RootUri.AbsoluteUri);

}}}
  Notice that the CrawlResult object does not have a collection of pages that were crawled. Keeping all the crawled pages in memory by default can cause a system out of memory exception if enough pages are crawled. So if you need to keep all the pages that were crawled in memory you must use the events above. 

===Changing Crawl Behavior===
The most common way to change a crawl is by either implementing ICrawlDecisionMaker or by extending CrawlDecisionMaker.
{{{
public interface ICrawlDecisionMaker
{
	/// <summary>
	/// Decides whether the page should be crawled
	/// </summary>
	CrawlDecision ShouldCrawlPage(PageToCrawl pageToCrawl, CrawlContext crawlContext);

	/// <summary>
	/// Decides whether the page's links should be crawled
	/// </summary>
	CrawlDecision ShouldCrawlPageLinks(CrawledPage crawledPage, CrawlContext crawlContext);

	/// <summary>
	/// Decides whether the page's content should be dowloaded
	/// </summary>
	CrawlDecision ShouldDownloadPageContent(CrawledPage crawledPage, CrawlContext crawlContext);
}
}}}