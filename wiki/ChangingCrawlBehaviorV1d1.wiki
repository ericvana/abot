#summary Changing the crawl behavior

=Changing the crawl behavior=

Abot was designed to be as pluggable as possible. This allows you to easily alter the way it works by default. There are several ways to alter the behavior.

===Altering By Configuration===
Like detailed in the quickstart. The easiest way to change Abot's behavior is to change the config values. No need to cover this in another section. See the QuickStart page for examples on the different ways Abot can be configured.

===Plugging In Custom Implementations===
Plug in your own implementations of key interfaces. 

{{{
PoliteWebCrawler crawler = new PoliteWebCrawler(
        new CrawlConfiguration(),
	new YourCrawlDecisionMaker(), //You will most likely write an impl for this
	new YourThreadMgr(), 
	new YourScheduler(), 
	new YourHttpRequester(), 
	new YourHyperLinkParser(), 
	new YourMemoryManager(), 
        new YourDomainRateLimiter,
	new YourRobotsDotTextFinder());
}}}

Passing null for any implementation will use the default. The example below will use your custom implementation for the IPageRequester and IHyperLinkParser but will use the default for all others.

{{{
PoliteWebCrawler crawler = new PoliteWebCrawler(
	null, 
	null, 
        null,
        null,
	new YourPageRequester(), 
	new YourHyperLinkParser(), 
	null,
        null, 
	null);
}}}

You will likely need to write your own implementation for ICrawlDecisionMaker since this is what will determine what pages get crawled, what pages will have their links crawled and whether the page's raw content should be downloaded. Also consider overriding !CrawlDecisionMaker.cs methods of interest to take advantage of some of its common behaviors like making sure a url is only crawled once.

{{{
public interface ICrawlDecisionMaker
{
	/// <summary>
	/// Decides whether the page should be crawled
	/// </summary>
	CrawlDecision ShouldCrawlPage(PageToCrawl pageToCrawl, CrawlContext crawlContext);

	/// <summary>
	/// Decides whether the page's links should be crawled
	/// </summary>
	CrawlDecision ShouldCrawlPageLinks(CrawledPage crawledPage, CrawlContext crawlContext);

	/// <summary>
	/// Decides whether the page's content should be downloaded
	/// </summary>
	CrawlDecision ShouldDownloadPageContent(CrawledPage crawledPage, CrawlContext crawlContext);
}
}}}

== Logging ==

Abot uses !Log4Net to log messages. See the app.config files in projects projects Abot.Demo or Abot.Tests.Integration to see how !Log4Net is configured. These log statements are a great way to see whats going on during a crawl. Below is from Abot.Demo project, app.config file. If this prints out to much data for you change the level from "DEBUG" to "INFO".

{{{
  <configSections>
    <section name="log4net" type="log4net.Config.Log4NetConfigurationSectionHandler, log4net" />
  </configSections>

  <log4net>
    <appender name="ConsoleAppender" type="log4net.Appender.ConsoleAppender">
      <layout type="log4net.Layout.PatternLayout">
        <conversionPattern value="[%date] [%thread] [%-5level] - %message%newline" />
      </layout>
    </appender>
    <appender name="RollingFileAppender" type="log4net.Appender.RollingFileAppender">
      <file value="log.txt" />
      <appendToFile value="true" />
      <rollingStyle value="Size" />
      <maxSizeRollBackups value="10" />
      <maximumFileSize value="10240KB" />
      <staticLogFileName value="true" />
      <layout type="log4net.Layout.PatternLayout">
        <conversionPattern value="[%date] [%-3thread] [%-5level] - %message%newline" />
      </layout>
    </appender>
    <root>
      <level value="DEBUG" />
      <appender-ref ref="ConsoleAppender" />
      <appender-ref ref="RollingFileAppender" />
    </root>
  </log4net>
}}}